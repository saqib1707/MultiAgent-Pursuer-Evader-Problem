{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_trajectory.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZOhSJzai1Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import hparams as hp\n",
        "# from tensorboardcolab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyrKeDWIi6se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data(data):\n",
        "    global data_mean, data_std\n",
        "    data_mean = np.mean(data[np.where(data.any(axis=1) != 0)[0]], axis=0)\n",
        "    data_std = np.std(data[np.where(data.any(axis=1) != 0)[0]], axis=0)\n",
        "    data[np.where(data.any(axis=1) != 0)[0]] = (data[np.where(data.any(axis=1) != 0)[0]] - data_mean)/data_std\n",
        "\n",
        "    # saving the normalized data in csv format for visualization/analysis\n",
        "    # np.savetxt(\"normalized_dataset.csv\", data, delimiter=\",\", fmt='%1.5f')\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGDgdgbFi9AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(train_data_file_path, test_data_file_path):\n",
        "    train_data = []\n",
        "    test_data = []\n",
        "\n",
        "    with open(train_data_file_path) as csvfile:\n",
        "        readCSV = csv.reader(csvfile, delimiter=',')\n",
        "        for row in readCSV:\n",
        "            train_data.append(row)\n",
        "\n",
        "    with open(test_data_file_path) as csvfile:\n",
        "        readCSV = csv.reader(csvfile, delimiter=',')\n",
        "        for row in readCSV:\n",
        "            test_data.append(row)\n",
        "    \n",
        "    train_data = np.array(train_data, dtype=np.float32)\n",
        "    test_data = np.array(test_data, dtype=np.float32)\n",
        "\n",
        "    train_x = train_data[:, 0:hp.input_feature_size]\n",
        "    train_y = train_data[:, hp.input_feature_size:hp.input_feature_size+hp.num_outputs]\n",
        "    test_x = test_data[:, 0:hp.input_feature_size]\n",
        "    test_y = test_data[:, hp.input_feature_size:hp.input_feature_size+hp.num_outputs]\n",
        "\n",
        "    # print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
        "    return train_x, train_y, test_x, test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwX-DO7Vmk7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3KJz2tWi_ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_full_batch(data_x, data_y, step, zero_row_indices):\n",
        "    if step == 0:\n",
        "        samples_range = range(0, zero_row_indices[step])\n",
        "    else:\n",
        "        samples_range = range(zero_row_indices[step-1]+1, zero_row_indices[step])\n",
        "\n",
        "    batch_size = len(samples_range)-hp.window_size+1\n",
        "    batch_x = np.zeros((batch_size, hp.window_size, hp.feature_size))\n",
        "    batch_y = np.zeros((batch_size, hp.num_outputs))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        batch_x[i] = data_x[samples_range[0]+i:samples_range[0]+i+hp.window_size]\n",
        "        batch_y[i] = data_y[samples_range[0]+i+hp.window_size]\n",
        "\n",
        "    return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLR6XIF4jBpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_batch(data_x, data_y, step, zero_row_indices):\n",
        "    if step == 0:\n",
        "        samples_range = range(0, zero_row_indices[step])\n",
        "    else:\n",
        "        samples_range = range(zero_row_indices[step-1]+1, zero_row_indices[step])\n",
        "\n",
        "    batch_size = len(samples_range)-hp.time_window_size+1\n",
        "    batch_x = np.zeros((batch_size, hp.time_window_size, hp.input_feature_size))\n",
        "    batch_y = np.zeros((batch_size, hp.num_outputs))\n",
        "\n",
        "    for index in range(batch_size):\n",
        "        batch_x[index] = data_x[samples_range[0]+index:samples_range[0]+index+hp.time_window_size]\n",
        "        batch_y[index] = data_y[samples_range[0]+index+hp.time_window_size-1]\n",
        "\n",
        "    return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KChZBknsjDoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_evader_position(pursuer_next_position, pursuer_current_position, evader_current_position, vemax_repulsion):\n",
        "    ti = 1.0\n",
        "    K = 1.0\n",
        "    vemax_attraction = 0\n",
        "    pursuer_next_position = np.reshape(pursuer_next_position, (2,1))\n",
        "    pursuer_current_position = np.reshape(pursuer_current_position, (2,1))\n",
        "    evader_current_position = np.reshape(evader_current_position, (4,1))\n",
        "\n",
        "    pursuer_evader_vector = np.transpose(np.reshape(evader_current_position, (2,2))) - pursuer_current_position  # (2,2)\n",
        "    pursuer_velocity = (pursuer_next_position - pursuer_current_position)/ti    # (2,1)\n",
        "    pursuer_evader_distance = np.reshape(np.linalg.norm(pursuer_evader_vector, axis=0), (2,1))     # (2,1)\n",
        "    costheta = np.matmul(np.transpose(pursuer_evader_vector), pursuer_velocity)/(pursuer_evader_distance*np.linalg.norm(pursuer_velocity)) # (2,1)\n",
        "    repulsion_term = np.reshape(np.transpose(0.5*vemax_repulsion*np.transpose(np.exp(-K*pursuer_evader_distance)*(1+costheta))*\n",
        "        (pursuer_evader_vector/np.transpose(pursuer_evader_distance))), (4,1));\n",
        "    evader_next_position = evader_current_position + repulsion_term*ti\n",
        "    return evader_next_position"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHnL_KnGjHff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fc_layer(input_data, num_input_units, num_output_units, variable_scope_name, activation='linear'):\n",
        "    init = tf.contrib.layers.xavier_initializer(uniform=False, dtype=tf.float32)\n",
        "    \n",
        "    with tf.variable_scope(variable_scope_name, reuse=tf.AUTO_REUSE):\n",
        "        weight = tf.get_variable(name='w', shape=[num_input_units, num_output_units], initializer=init, trainable=True)\n",
        "        bias = tf.get_variable(name='b', shape=[num_output_units], initializer=init, trainable=True)\n",
        "    \n",
        "    fc_layer_output = tf.add(tf.matmul(tf.reshape(input_data, [-1, num_input_units]), weight), bias)\n",
        "    \n",
        "    if activation == 'relu':\n",
        "        fc_layer_output = tf.nn.relu(fc_layer_output)\n",
        "    elif activation == 'tanh':\n",
        "        fc_layer_output = tf.nn.tanh(fc_layer_output)\n",
        "    elif activation == 'sigmoid':\n",
        "        fc_layer_output = tf.nn.sigmoid(fc_layer_output)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    return fc_layer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onea__YR26aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def build_rnn_layers(rnn_inputs, cell_state_size, batch_size):\n",
        "#     \"\"\"\n",
        "#         rnn_inputs : [batch_size, max_time, input_feature_size]\n",
        "#         initial_state : [batch_size, cell_state_size]\n",
        "#         final_state : [batch_size, cell_state_size]\n",
        "#         outputs : [batch_size, max_time, cell_state_size]\n",
        "#     \"\"\"\n",
        "#     # create a BasicRNNCell\n",
        "#     rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=cell_state_size, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "#     # defining initial state\n",
        "#     initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "#     outputs, final_state = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=rnn_inputs, initial_state=initial_state, dtype=tf.float32)\n",
        "\n",
        "#     return outputs, final_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uR2Z5VJjFlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_lstm_layers(lstm_inputs, cell_state_sizes, batch_size, keep_prob_):\n",
        "    lstm_cells = [tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=size) for size in cell_state_sizes]\n",
        "    # Add dropout to the cell\n",
        "    dropout_lstms = [tf.nn.rnn_cell.DropoutWrapper(cell=lstm, output_keep_prob=keep_prob_) for lstm in lstm_cells]\n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    stacked_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cells=dropout_lstms)\n",
        "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell=stacked_rnn_cell, inputs=lstm_inputs, dtype=tf.float32)\n",
        "    return lstm_outputs, final_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBw1BbotjJiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def core_model(inputs, cell_state_sizes, batch_size, keep_prob):\n",
        "    # rnn_outputs, final_state = build_rnn_layers(inputs, cell_state_sizes, batch_size)\n",
        "    rnn_outputs, final_state = build_lstm_layers(inputs[:,:,0:6], cell_state_sizes, batch_size, keep_prob)\n",
        "    rnn_final_output = rnn_outputs[:,-1,:]      # taking the final state output (many-to-one)\n",
        "    num_input_units = cell_state_sizes[-1]+2\n",
        "\n",
        "    snn_input = tf.concat([rnn_final_output, inputs[:,0,6:8]], 1)\n",
        "    prev_layer = tf.reshape(snn_input, [-1, num_input_units])\n",
        "\n",
        "    for l in range(len(hp.num_fc_units)):\n",
        "        if l == len(hp.num_fc_units)-1:\n",
        "            activation = 'linear'\n",
        "        else:\n",
        "            activation = 'relu'\n",
        "\n",
        "        next_layer = create_fc_layer(prev_layer, num_input_units, hp.num_fc_units[l], 'FC_'+str(l), activation)\n",
        "        prev_layer = next_layer\n",
        "        num_input_units = hp.num_fc_units[l]\n",
        "\n",
        "    predicted_output = next_layer\n",
        "    return predicted_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfir-vy0jLDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_clip(grads, max_grad_norm):\n",
        "    clipped_grads, global_norm = tf.clip_by_global_norm(t_list=grads, clip_norm=max_grad_norm)  # (t_list*clip_norm)/max(clip_norm, global_norm)\n",
        "    # grad_norm_summary = [tf.summary.scalar(\"grad_norm\", global_norm)]\n",
        "    # grad_norm_summary.append(tf.summary.scalar(\"clipped_gradient\", tf.global_norm(clipped_grads)))\n",
        "    return clipped_grads, global_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So14NSLLksgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_evader_position_tf(pursuer_next_position, pursuer_current_position, evader_current_position, vemax_repulsion):\n",
        "    ti = 1.0\n",
        "    K = 1.0\n",
        "    vemax_attraction = 0\n",
        "    pursuer_next_position = tf.reshape(pursuer_next_position, [2,1])\n",
        "    pursuer_current_position = tf.reshape(pursuer_current_position, [2,1])\n",
        "    evader_current_position = tf.reshape(evader_current_position, [4,1])\n",
        "\n",
        "    pursuer_evader_vector = tf.transpose(tf.reshape(evader_current_position, [2,2])) - pursuer_current_position  # (2,2)\n",
        "    pursuer_velocity = (pursuer_next_position - pursuer_current_position)/ti    # (2,1)\n",
        "    pursuer_evader_distance = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(pursuer_evader_vector), axis=0)), [2,1])     # (2,1)\n",
        "    pursuer_speed = tf.sqrt(tf.reduce_sum(tf.square(pursuer_velocity)))\n",
        "    costheta = tf.matmul(tf.transpose(pursuer_evader_vector), pursuer_velocity)/(pursuer_evader_distance*pursuer_speed) # (2,1)\n",
        "    repulsion_term = tf.reshape(tf.transpose(0.5*vemax_repulsion*tf.transpose(tf.exp(-K*pursuer_evader_distance)*(1+costheta))*\n",
        "        (pursuer_evader_vector/tf.transpose(pursuer_evader_distance))), [4,1])\n",
        "    evader_next_position = tf.add(evader_current_position, repulsion_term*ti)\n",
        "    return evader_next_position"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcMrSqkjM2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(train_x, train_y):\n",
        "    network_input = tf.placeholder(dtype=tf.float32, shape=[None, hp.time_window_size, hp.input_feature_size], name='input_placeholder')\n",
        "    network_output = tf.placeholder(dtype=tf.float32, shape=[None, hp.num_outputs], name='output_placeholder')   # ground-truth\n",
        "    batch_size_placeholder = tf.placeholder(dtype=tf.int32, shape=(), name='batch_size')\n",
        "    keep_prob = tf.placeholder_with_default(input=1.0, shape=())\n",
        "\n",
        "    num_training_samples = train_x.shape[0]\n",
        "    zero_row_indices = np.where(train_x.any(axis=1) == 0)[0]\n",
        "    num_zero_row_indices = num_batches = zero_row_indices.shape[0]\n",
        "\n",
        "    predicted_pursuer_position = core_model(network_input, hp.cell_state_sizes, batch_size_placeholder, keep_prob)    # predicted_output : [batch_size,2]\n",
        "    predicted_pursuer_position = tf.reshape(predicted_pursuer_position, [-1, hp.num_outputs])\n",
        "    # evader_final_position = compute_evader_position_tf(predicted_pursuer_position[-1,:], network_input[-1,-1,0:2], network_input[-1,-1,2:6], 0.4)\n",
        "\n",
        "    # evader_positions = []\n",
        "    # for i in range(batch_size_placeholder.eval()):\n",
        "    #     evader_next_position = compute_evader_position_tf(predicted_pursuer_position)\n",
        "\n",
        "    destination_loss = 0\n",
        "    # destination_loss = tf.reduce_sum(tf.sqrt(tf.reduce_sum(tf.square(tf.transpose(tf.reshape(evader_final_position, [2,2])) - \n",
        "    #     tf.reshape(network_input[-1,-1,6:8], [2,1])), axis=0)))\n",
        "\n",
        "    pursuer_evader_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(network_output, predicted_pursuer_position)), axis=1))\n",
        "\n",
        "    batch_loss = tf.add(pursuer_evader_loss, destination_loss)\n",
        "    tf.summary.scalar('mean_squared_loss', batch_loss)\n",
        "\n",
        "    # optimizer.minimize(total_loss) = optimizer.compute_gradients() -> optimizer.apply_gradients()\n",
        "    optimizer = tf.train.AdamOptimizer(hp.learning_rate)\n",
        "\n",
        "    with tf.name_scope(\"compute_gradients\"):\n",
        "        grads_and_params = optimizer.compute_gradients(loss=batch_loss)\n",
        "        grads, params = zip(*grads_and_params)\n",
        "        # for var in params:\n",
        "        #     tf.summary.histogram(var.name, var)\n",
        "        clipped_grads, global_norm = gradient_clip(grads, hp.max_gradient_norm)\n",
        "        grads_and_params = zip(clipped_grads, params)\n",
        "\n",
        "    \"\"\"\n",
        "        global_step refers to the number of batches seen by the graph. Every time a batch is provided, the weights are updated in the direction \n",
        "        that minimizes the loss. global_step just keeps track of the number of batches seen so far\n",
        "    \"\"\"\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    apply_gradient_op = optimizer.apply_gradients(grads_and_vars=grads_and_params, global_step=global_step)\n",
        "\n",
        "    session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    session_config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=session_config)\n",
        "    train_writer = tf.summary.FileWriter(hp.summary_dir, sess.graph)\n",
        "\n",
        "    # This initializer is designed to keep the scale of the gradients roughly the same in all layers\n",
        "    initializer = tf.contrib.layers.xavier_initializer(uniform=False, dtype=tf.float32)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.variable_scope(tf.get_variable_scope(), initializer=initializer):\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        merged_summary = tf.summary.merge_all()\n",
        "\n",
        "        start_time = time.time()\n",
        "        print('Training Start\\n')\n",
        "\n",
        "        for epoch in range(hp.num_epochs):\n",
        "            print(\"Training:: epoch:\", epoch)\n",
        "            for step in range(num_batches):\n",
        "                [batch_x, batch_y] = get_next_batch(train_x, train_y, step, zero_row_indices)\n",
        "\n",
        "                # ppp = np.array(sess.run(predicted_pursuer_position,\n",
        "                #     feed_dict={\n",
        "                #         network_input: batch_x,\n",
        "                #         batch_size_placeholder: batch_x.shape[0],\n",
        "                #         keep_prob: hp.keep_prob_\n",
        "                #     }\n",
        "                # ))\n",
        "\n",
        "                # for i in range(batch_x.shape[0]):\n",
        "                #     enp = compute_evader_position(ppp[i], batch_x[i, hp.time_window_size-1, 0:2], batch_x[i, hp.time_window_size-1, 2:6], 0.3)\n",
        "\n",
        "                # dest_loss = np.sqrt(np.sum(np.square(enp[0:2,0]-batch_x[0,0,6:8]))) + np.sqrt(np.sum(np.square(enp[2:4,0]-batch_x[0,0,6:8])))\n",
        "\n",
        "                _, iter_count = sess.run([apply_gradient_op, global_step],\n",
        "                    feed_dict={\n",
        "                        network_input: batch_x,\n",
        "                        network_output: batch_y,\n",
        "                        batch_size_placeholder: batch_x.shape[0],\n",
        "                        keep_prob: hp.keep_prob_\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                loss_val, summary = sess.run([batch_loss, merged_summary],\n",
        "                    feed_dict={\n",
        "                        network_input: batch_x,\n",
        "                        network_output: batch_y,\n",
        "                        batch_size_placeholder: batch_x.shape[0],\n",
        "                        keep_prob: hp.keep_prob_\n",
        "                    }\n",
        "                )\n",
        "                train_writer.add_summary(summary, iter_count)\n",
        "\n",
        "        print('\\nTraining End')\n",
        "        print('Training Time: ', time.time()-start_time)\n",
        "\n",
        "        if os.path.isdir(hp.model_base_dir) == False:\n",
        "            os.mkdir(hp.model_base_dir)\n",
        "        if os.path.isdir(hp.model_save_dir) == False:\n",
        "            os.mkdir(hp.model_save_dir)\n",
        "\n",
        "        save_path = saver.save(sess, hp.model_save_dir+hp.model_save_filename+'.ckpt')\n",
        "        print('Model saved in path: %s' % save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZwtzjzrk0ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_network(test_x, test_y):\n",
        "    print('\\nPerfomance on test data\\n')\n",
        "    tf.reset_default_graph()\n",
        "    network_input = tf.placeholder(dtype=tf.float32, shape=[None, hp.time_window_size, hp.input_feature_size], name='input_placeholder')\n",
        "    network_output = tf.placeholder(dtype=tf.float32, shape=[None, hp.num_outputs], name='output_placeholder')   # ground-truth\n",
        "    batch_size_placeholder = tf.placeholder(dtype=tf.int32, shape=(), name='batch_size')\n",
        "    keep_prob = tf.placeholder_with_default(input=1.0, shape=())\n",
        "\n",
        "    zero_row_indices = np.where(test_x.any(axis=1) == 0)[0]\n",
        "    num_zero_row_indices = num_batches = zero_row_indices.shape[0]\n",
        "\n",
        "    predicted_pursuer_position = core_model(network_input, hp.cell_state_sizes, batch_size_placeholder, keep_prob)    # predicted_output : [batch_size, num_outputs]\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # Restore variables from disk\n",
        "        saver.restore(sess, hp.model_save_dir+hp.model_save_filename+'.ckpt')\n",
        "        print('Model Restored')\n",
        "\n",
        "        for k in range(num_batches):\n",
        "            if k == 0:\n",
        "                num_interval = zero_row_indices[k] - hp.time_window_size + 1\n",
        "                test_batch_x = np.reshape(np.copy(test_x[0:hp.time_window_size]), (1, hp.time_window_size, hp.input_feature_size))\n",
        "            else:\n",
        "                num_interval = (zero_row_indices[k] - zero_row_indices[k-1]) - hp.time_window_size\n",
        "                test_batch_x = np.reshape(np.copy(test_x[zero_row_indices[k-1]+1:zero_row_indices[k-1]+1+hp.time_window_size]), \\\n",
        "                    (1, hp.time_window_size, hp.input_feature_size))\n",
        "\n",
        "            print('Index::', k, 'Pursuer Position::', test_batch_x[0,0,0:2], 'Destination::', test_batch_x[0,0,6:8])\n",
        "\n",
        "            pursuer_trajectory = []\n",
        "            evader_trajectory = []\n",
        "\n",
        "            for i in range(hp.time_window_size):\n",
        "                pursuer_trajectory.append(np.reshape(np.copy(test_batch_x[0,i,0:2]), (2,1)))\n",
        "                evader_trajectory.append(np.reshape(np.copy(test_batch_x[0,i,2:6]), (4,1)))\n",
        "\n",
        "            for j in range(num_interval):\n",
        "                pursuer_next_position = sess.run([predicted_pursuer_position], \n",
        "                    feed_dict={\n",
        "                        network_input: test_batch_x,\n",
        "                        batch_size_placeholder: test_batch_x.shape[0],\n",
        "                        keep_prob: 1.0\n",
        "                    }\n",
        "                )\n",
        "                pursuer_next_position = np.reshape(pursuer_next_position, (2,1))\n",
        "                evader_next_position = compute_evader_position(pursuer_next_position, pursuer_trajectory[j+hp.time_window_size-1], \n",
        "                    evader_trajectory[j+hp.time_window_size-1], 0.4)\n",
        "\n",
        "                pursuer_trajectory.append(pursuer_next_position)\n",
        "                evader_trajectory.append(evader_next_position)\n",
        "\n",
        "                test_batch_x[0, 0:hp.time_window_size-1, :] = np.copy(test_batch_x[0, 1:hp.time_window_size, :])\n",
        "                test_batch_x[0, hp.time_window_size-1, 0:2] = np.reshape(np.copy(pursuer_next_position), (2))\n",
        "                test_batch_x[0, hp.time_window_size-1, 2:6] = np.reshape(np.copy(evader_next_position), (4))\n",
        "\n",
        "            pursuer_trajectory = np.transpose(np.array(pursuer_trajectory)[:,:,0])  # (2,number_interval)\n",
        "            evader_trajectory = np.transpose(np.array(evader_trajectory)[:,:,0])    # (4,number_interval)\n",
        "\n",
        "            # print(pursuer_trajectory.shape, evader_trajectory.shape)\n",
        "\n",
        "            plt.plot(pursuer_trajectory[0,:], pursuer_trajectory[1,:], 'blue', lw=0.5, marker='.', label='pursuer_trajectory')\n",
        "            plt.plot(evader_trajectory[0,:], evader_trajectory[1,:], 'red', lw=0.5, marker='.', label='evader1_trajectory')\n",
        "            plt.plot(evader_trajectory[2,:], evader_trajectory[3,:], 'red', lw=0.5, marker='.', label='evader2_trajectory')\n",
        "\n",
        "            destination = test_batch_x[0,0,6:8]\n",
        "            draw_circle(destination[0], destination[1], hp.epsilon)\n",
        "\n",
        "            plt.title('Shepherding-prediction-result')\n",
        "            plt.xlabel('X')\n",
        "            plt.ylabel('Y')\n",
        "            plt.grid(True)\n",
        "            plt.legend()\n",
        "\n",
        "            if k == 0:\n",
        "                test_batch_x = np.reshape(np.copy(test_x[0:hp.time_window_size]), (1, hp.time_window_size, hp.input_feature_size))\n",
        "            else:\n",
        "                test_batch_x = np.reshape(np.copy(test_x[zero_row_indices[k-1]+1:zero_row_indices[k-1]+1+hp.time_window_size]), \\\n",
        "                    (1, hp.time_window_size, hp.input_feature_size))\n",
        "\n",
        "            plt_save_filename = str(test_batch_x[0,0,0])+'_'+str(test_batch_x[0,0,1])+'_'+str(test_batch_x[0,0,6])+'_'+ \\\n",
        "            str(test_batch_x[0,0,7])+'.png'\n",
        "\n",
        "            if os.path.isdir(hp.plt_base_dir) == False:\n",
        "                os.mkdir(hp.plt_base_dir)\n",
        "            if os.path.isdir(hp.plt_save_dir) == False:\n",
        "                os.mkdir(hp.plt_save_dir)\n",
        "            plt.savefig(hp.plt_save_dir+plt_save_filename)\n",
        "            plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYYEGgQd3Fcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_compute_evader_function(data_file_path):\n",
        "    data = []\n",
        "    with open(data_file_path) as csvfile:\n",
        "        readCSV = csv.reader(csvfile, delimiter=',')\n",
        "        for row in readCSV:\n",
        "            data.append(row)\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "\n",
        "    initial_index = 65\n",
        "    end_index = 95\n",
        "\n",
        "    pursuer_trajectory = np.transpose(data[initial_index-1:end_index+1, 0:2])\n",
        "    initial_evader_position = np.transpose(data[initial_index-1,2:6])\n",
        "    evader_trajectory = []\n",
        "    evader_trajectory.append(np.reshape(initial_evader_position,(4,1)))\n",
        "\n",
        "    # sess = tf.Session()\n",
        "    # evader_trajectory_tf = []\n",
        "    # evader_trajectory_tf.append(np.reshape(initial_evader_position,(4,1)))\n",
        "\n",
        "    for index in range(end_index - initial_index):\n",
        "        evader_trajectory.append(compute_evader_position(pursuer_trajectory[:,index+1], pursuer_trajectory[:,index], \n",
        "            evader_trajectory[index], 0.5))\n",
        "        # evader_trajectory_tf.append(sess.run(compute_evader_position_tf(pursuer_trajectory[:,index+1], pursuer_trajectory[:,index], \\\n",
        "        #     evader_trajectory[index], 0.5)))\n",
        "\n",
        "    evader_trajectory = np.array(evader_trajectory)[:,:,0]\n",
        "    # evader_trajectory_tf = np.array(evader_trajectory_tf)[:,:,0]\n",
        "\n",
        "    # print(evader_trajectory - evader_trajectory_tf)\n",
        "\n",
        "    plt.plot(pursuer_trajectory[0,:-1],pursuer_trajectory[1,:-1], 'blue', lw=1.0, marker='.')\n",
        "\n",
        "    # plt.plot(evader_trajectory_tf[:,0],evader_trajectory_tf[:,1], 'brown', lw=1.0, marker='.')\n",
        "    # plt.plot(evader_trajectory_tf[:,2],evader_trajectory_tf[:,3], 'brown', lw=1.0, marker='.')\n",
        "\n",
        "    plt.plot(evader_trajectory[:,0],evader_trajectory[:,1], 'green', lw=1.0, marker='.')\n",
        "    plt.plot(evader_trajectory[:,2],evader_trajectory[:,3], 'green', lw=1.0, marker='.')\n",
        "\n",
        "    draw_circle(data[initial_index-1, 6], data[initial_index-1, 7], 0.05)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCKrTr9AjPvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_circle(x_center, y_center, radius, angle_resolution=0.1):\n",
        "    angles = np.linspace(0, 2*np.pi, 360/angle_resolution)\n",
        "    xp = radius*np.cos(angles)\n",
        "    yp = radius*np.sin(angles)\n",
        "    plt.plot(x_center+xp, y_center+yp, 'black')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zY02BNfjRmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # check_compute_evader_function(hp.train_data_file_path)\n",
        "    train_x, train_y, test_x, test_y = get_data(hp.train_data_file_path, hp.test_data_file_path)\n",
        "    \n",
        "    if os.path.isdir(hp.summary_dir) == True:\n",
        "        shutil.rmtree(hp.summary_dir)\n",
        "        os.makedirs(hp.summary_dir)\n",
        "    else:\n",
        "        os.makedirs(hp.summary_dir)\n",
        "\n",
        "    if hp.train_bool == True:\n",
        "        train_network(train_x, train_y)\n",
        "    if hp.test_bool == True:\n",
        "        test_network(test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FikHwKlqNh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# !zip -r /content/save_model_dir_1707.zip /content/save_model_dir/\n",
        "# files.download('save_model_dir_1707.zip')\n",
        "\n",
        "# !zip -r /content/results_plots_1707.zip /content/results_plots/\n",
        "# files.download('results_plots_1707.zip')\n",
        "\n",
        "# !zip -r /content/tensorboard_graph_1707.zip /content/Graph/\n",
        "# files.download('tensorboard_graph_1707.zip')\n",
        "\n",
        "# !mkdir data/\n",
        "# !mv dataset_train.csv data/\n",
        "# !rm -r sample_data/\n",
        "# !rm *.zip\n",
        "# !rm -r results_plots/\n",
        "# !rm -r save_model_dir/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}